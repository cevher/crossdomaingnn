{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c888fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch_geometric.data import HeteroData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5751091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cihaz: cuda | Model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# AYARLAR VE SABİTLER\n",
    "# ==========================================\n",
    "PATHS = {\n",
    "    'source_inter': 'Books.jsonl.gz',\n",
    "    'source_meta': 'meta_Books.jsonl.gz',\n",
    "    'target_inter': 'Electronics.jsonl.gz',\n",
    "    'target_meta': 'meta_Electronics.jsonl.gz'\n",
    "}\n",
    "\n",
    "# Min-Core filtrelemesi (Her kullanıcının en az N etkileşimi olsun)\n",
    "MIN_INTERACTIONS = 10 \n",
    "\n",
    "# Kullanılacak Dil Modeli (Hızlı ve etkili)\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2' \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Cihaz: {device} | Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab8fce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# YARDIMCI FONKSİYONLAR\n",
    "# ==========================================\n",
    "\n",
    "def get_users_from_file(filepath):\n",
    "    \"\"\"Dosyadan tüm kullanıcı ID'lerini set olarak döner.\"\"\"\n",
    "    users = set()\n",
    "    with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=f\"Kullanıcılar taranıyor: {filepath}\"):\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                users.add(data['user_id'])\n",
    "            except: continue\n",
    "    return users\n",
    "\n",
    "def load_filtered_interactions(filepath, shared_users, domain_name):\n",
    "    \"\"\"Sadece ortak kullanıcılara ait etkileşimleri DataFrame olarak döner.\"\"\"\n",
    "    data_list = []\n",
    "    with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=f\"{domain_name} verisi yükleniyor\"):\n",
    "            try:\n",
    "                d = json.loads(line)\n",
    "                if d['user_id'] in shared_users:\n",
    "                    data_list.append({\n",
    "                        'user_id': d['user_id'],\n",
    "                        'item_id': d['parent_asin'], # Amazon 2023 standardı\n",
    "                        'rating': float(d['rating']),\n",
    "                        'timestamp': d.get('timestamp', 0)\n",
    "                    })\n",
    "            except: continue\n",
    "    \n",
    "    df = pd.DataFrame(data_list)\n",
    "    # Min-Core Filtreleme (Gürültüyü azaltmak için)\n",
    "    user_counts = df['user_id'].value_counts()\n",
    "    valid_users = user_counts[user_counts >= MIN_INTERACTIONS].index\n",
    "    df = df[df['user_id'].isin(valid_users)]\n",
    "    \n",
    "    print(f\"[{domain_name}] Yüklenen Etkileşim: {len(df)} | Unique User: {df['user_id'].nunique()} | Unique Item: {df['item_id'].nunique()}\")\n",
    "    return df\n",
    "\n",
    "def generate_item_features(meta_path, item_ids_set, text_model):\n",
    "    \"\"\"\n",
    "    Ürünlerin meta verilerini okur ve başlıklarını BERT ile encode eder.\n",
    "    Sadece etkileşim verisinde geçen (item_ids_set) ürünleri işler.\n",
    "    \"\"\"\n",
    "    item_titles = {}\n",
    "    item_categories = {} # İstersen kategori de ekleyebilirsin\n",
    "    \n",
    "    with gzip.open(meta_path, 'rt', encoding='utf-8') as f:\n",
    "        for line in tqdm(f, desc=\"Meta veri taranıyor\"):\n",
    "            try:\n",
    "                d = json.loads(line)\n",
    "                asin = d['parent_asin']\n",
    "                if asin in item_ids_set:\n",
    "                    # Başlık yoksa boş string ata\n",
    "                    title = d.get('title', '') + \" \" + \" \".join(d.get('features', []))\n",
    "                    item_titles[asin] = title\n",
    "            except: continue\n",
    "            \n",
    "    # Sıralamayı garanti altına alalım (DataFrame indexiyle uyumlu olmalı)\n",
    "    # Bu fonksiyon dışarıdan gelen sıralı item listesine göre embedding üretmeli\n",
    "    # O yüzden burada sadece map döndürüyoruz, aşağıda sıralayacağız.\n",
    "    return item_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a044aa84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 1. Adım: Ortak Kullanıcılar Bulunuyor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Kullanıcılar taranıyor: Books.jsonl.gz: 29475453it [03:38, 135024.47it/s]\n",
      "Kullanıcılar taranıyor: Electronics.jsonl.gz: 43886944it [03:53, 187731.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam Ortak Kullanıcı Sayısı: 4460556\n",
      "\n",
      "--- 2. Adım: Etkileşimler Yükleniyor ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Source (Books) verisi yükleniyor: 29475453it [02:43, 180313.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Source (Books)] Yüklenen Etkileşim: 6581746 | Unique User: 263761 | Unique Item: 1900915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target (Electronics) verisi yükleniyor: 43886944it [03:43, 196024.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Target (Electronics)] Yüklenen Etkileşim: 5528803 | Unique User: 289472 | Unique Item: 690103\n",
      "Min-Core (10) sonrası Final Ortak Kullanıcı: 45085\n",
      "\n",
      "--- 3. Adım: Indexleme Yapılıyor ---\n",
      "\n",
      "--- 4. Adım: Ürün Özellikleri Çıkarılıyor (BERT) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1a92576f0714138b33d3b285de6ef42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\efeoz\\anaconda3\\envs\\llms\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\efeoz\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75de7fd5a2aa4d1fa012d095c0f6294f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4838922274a497497f016b932307060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4bbcd3ffbd44241a1b6f3c86a694769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9c6b9ac97549ad9aeff41185d82c3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99601fad238d451aafe47cec2baa8752",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1155ac4f9ade46caa993b4b6666443bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bf115edb9474362a73bd20d566e4ef3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b1e23ab273e4758ad5710e1ecd2e89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81cf6171bad34e1a968f942453786d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3803ab4547d943f587229bb1a53f590f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# ANA AKIŞ (PIPELINE)\n",
    "# ==========================================\n",
    "\n",
    "# 1. Adım: Ortak Kullanıcıları Bulma\n",
    "print(\"\\n--- 1. Adım: Ortak Kullanıcılar Bulunuyor ---\")\n",
    "source_users = get_users_from_file(PATHS['source_inter'])\n",
    "target_users = get_users_from_file(PATHS['target_inter'])\n",
    "shared_users = source_users.intersection(target_users)\n",
    "print(f\"Toplam Ortak Kullanıcı Sayısı: {len(shared_users)}\")\n",
    "\n",
    "# 2. Adım: Etkileşim Verilerini Yükleme\n",
    "print(\"\\n--- 2. Adım: Etkileşimler Yükleniyor ---\")\n",
    "source_df = load_filtered_interactions(PATHS['source_inter'], shared_users, \"Source (Books)\")\n",
    "target_df = load_filtered_interactions(PATHS['target_inter'], shared_users, \"Target (Electronics)\")\n",
    "\n",
    "# Filtreleme sonrası ortak kullanıcıları tekrar güncelle (Min-core sonrası düşenler olabilir)\n",
    "final_shared_users = set(source_df['user_id']).intersection(set(target_df['user_id']))\n",
    "source_df = source_df[source_df['user_id'].isin(final_shared_users)]\n",
    "target_df = target_df[target_df['user_id'].isin(final_shared_users)]\n",
    "print(f\"Min-Core ({MIN_INTERACTIONS}) sonrası Final Ortak Kullanıcı: {len(final_shared_users)}\")\n",
    "\n",
    "# 3. Adım: ID Mapping (Global User ID, Local Item ID)\n",
    "print(\"\\n--- 3. Adım: Indexleme Yapılıyor ---\")\n",
    "# Kullanıcılar için global harita\n",
    "user_map = {uid: i for i, uid in enumerate(final_shared_users)}\n",
    "\n",
    "# Ürünler için domain bazlı harita\n",
    "source_item_map = {iid: i for i, iid in enumerate(source_df['item_id'].unique())}\n",
    "target_item_map = {iid: i for i, iid in enumerate(target_df['item_id'].unique())}\n",
    "\n",
    "# 4. Adım: Feature Extraction (BERT Embeddings)\n",
    "print(\"\\n--- 4. Adım: Ürün Özellikleri Çıkarılıyor (BERT) ---\")\n",
    "text_model = SentenceTransformer(MODEL_NAME, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3af9af2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta veri taranıyor: 4448181it [01:36, 46135.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metinler encode ediliyor...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6748dbf0024b4f8595bff7b36d15b2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/20810 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Meta veri taranıyor: 1610012it [00:38, 41883.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metinler encode ediliyor...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f091e7fd02d45de82692da05b983670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/8753 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Adım: Grafik İnşa Ediliyor ---\n"
     ]
    }
   ],
   "source": [
    "def get_ordered_embeddings(item_map, meta_path):\n",
    "    # Meta dosyasından başlıkları çek\n",
    "    raw_titles = generate_item_features(meta_path, set(item_map.keys()), text_model)\n",
    "    \n",
    "    # item_map sırasına göre listeyi oluştur\n",
    "    ordered_titles = []\n",
    "    for iid in item_map.keys():\n",
    "        ordered_titles.append(raw_titles.get(iid, \"Unknown Product\")) # Başlık yoksa placeholder\n",
    "    \n",
    "    # Batch halinde encode et\n",
    "    print(\"Metinler encode ediliyor...\")\n",
    "    embeddings = text_model.encode(ordered_titles, convert_to_tensor=True, show_progress_bar=True)\n",
    "    return embeddings\n",
    "\n",
    "# Source Item Features\n",
    "x_source_item = get_ordered_embeddings(source_item_map, PATHS['source_meta'])\n",
    "# Target Item Features\n",
    "x_target_item = get_ordered_embeddings(target_item_map, PATHS['target_meta'])\n",
    "\n",
    "# User Features (Learnable Embedding Başlangıcı)\n",
    "# Q1 için not: Kullanıcılar için interaction geçmişlerinin ortalamasını almak daha iyidir\n",
    "# Ancak şimdilik learnable embedding ile başlatıyoruz, eğitimde güncellenecek.\n",
    "x_user = torch.nn.Embedding(len(user_map), x_source_item.shape[1]).weight.data # Aynı boyutta başlat\n",
    "\n",
    "# 5. Adım: PyG HeteroData Oluşturma\n",
    "print(\"\\n--- 5. Adım: Grafik İnşa Ediliyor ---\")\n",
    "data = HeteroData()\n",
    "\n",
    "# --- DÜĞÜMLER (NODES) ---\n",
    "data['user'].x = x_user\n",
    "data['book'].x = x_source_item\n",
    "data['elec'].x = x_target_item\n",
    "\n",
    "data['user'].num_nodes = len(user_map)\n",
    "data['book'].num_nodes = len(source_item_map)\n",
    "data['elec'].num_nodes = len(target_item_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3ff30f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- GRAFİK ÖZETİ ---\n",
      "HeteroData(\n",
      "  user={\n",
      "    x=[45085, 384],\n",
      "    num_nodes=45085,\n",
      "  },\n",
      "  book={\n",
      "    x=[665897, 384],\n",
      "    num_nodes=665897,\n",
      "  },\n",
      "  elec={\n",
      "    x=[280080, 384],\n",
      "    num_nodes=280080,\n",
      "  },\n",
      "  (user, rates, book)={\n",
      "    edge_index=[2, 1422281],\n",
      "    edge_attr=[1422281],\n",
      "  },\n",
      "  (user, rates, elec)={\n",
      "    edge_index=[2, 1067270],\n",
      "    edge_attr=[1067270],\n",
      "  },\n",
      "  (book, rated_by, user)={\n",
      "    edge_index=[2, 1422281],\n",
      "    edge_attr=[1422281],\n",
      "  },\n",
      "  (elec, rated_by, user)={\n",
      "    edge_index=[2, 1067270],\n",
      "    edge_attr=[1067270],\n",
      "  }\n",
      ")\n",
      "Source Item Feature Shape: torch.Size([665897, 384])\n",
      "Target Item Feature Shape: torch.Size([280080, 384])\n"
     ]
    }
   ],
   "source": [
    "# --- KENARLAR (EDGES) ---\n",
    "def create_edge_tensor(df, u_map, i_map, item_col):\n",
    "    src = [u_map[u] for u in df['user_id']]\n",
    "    dst = [i_map[i] for i in df[item_col]]\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(df['rating'].values, dtype=torch.float)\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "# User -> Book\n",
    "edge_index_ub, edge_attr_ub = create_edge_tensor(source_df, user_map, source_item_map, 'item_id')\n",
    "data['user', 'rates', 'book'].edge_index = edge_index_ub\n",
    "data['user', 'rates', 'book'].edge_attr = edge_attr_ub\n",
    "\n",
    "# User -> Elec\n",
    "edge_index_ue, edge_attr_ue = create_edge_tensor(target_df, user_map, target_item_map, 'item_id')\n",
    "data['user', 'rates', 'elec'].edge_index = edge_index_ue\n",
    "data['user', 'rates', 'elec'].edge_attr = edge_attr_ue\n",
    "\n",
    "# Ters Kenarlar (GNN Mesaj İletimi İçin Şart)\n",
    "data['book', 'rated_by', 'user'].edge_index = edge_index_ub[[1, 0]]\n",
    "data['book', 'rated_by', 'user'].edge_attr = edge_attr_ub\n",
    "\n",
    "data['elec', 'rated_by', 'user'].edge_index = edge_index_ue[[1, 0]]\n",
    "data['elec', 'rated_by', 'user'].edge_attr = edge_attr_ue\n",
    "\n",
    "print(\"\\n--- GRAFİK ÖZETİ ---\")\n",
    "print(data)\n",
    "print(f\"Source Item Feature Shape: {data['book'].x.shape}\")\n",
    "print(f\"Target Item Feature Shape: {data['elec'].x.shape}\")\n",
    "\n",
    "# Kaydetme (Opsiyonel)\n",
    "# torch.save(data, 'amazon_cdr_graph.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "757ebae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, HeteroConv, Linear\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.loader import LinkNeighborLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51931ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veri Train/Val/Test olarak bölünüyor...\n",
      "Eğitim Kenar Sayısı (Elec): 853816\n",
      "Test Kenar Sayısı (Elec): 960543\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. VERİ BÖLME (TRAIN / VAL / TEST)\n",
    "# ==========================================\n",
    "print(\"Veri Train/Val/Test olarak bölünüyor...\")\n",
    "\n",
    "# Sadece Hedef Domain (Electronics) kenarlarını bölüyoruz\n",
    "# Kaynak Domain (Books) eğitimde tamamen kullanılıyor (Full Knowledge Transfer)\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    edge_types=[('user', 'rates', 'elec')], # Sadece hedefi böl\n",
    "    rev_edge_types=[('elec', 'rated_by', 'user')],\n",
    "    is_undirected=False,\n",
    "    add_negative_train_samples=False # Rating prediction (Regression) için negatif örneklemeye gerek yok\n",
    ")\n",
    "\n",
    "train_data, val_data, test_data = transform(data)\n",
    "print(f\"Eğitim Kenar Sayısı (Elec): {train_data['user', 'rates', 'elec'].edge_index.size(1)}\")\n",
    "print(f\"Test Kenar Sayısı (Elec): {test_data['user', 'rates', 'elec'].edge_index.size(1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a39e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. DATA LOADERS (OPTİMİZE EDİLMİŞ)\n",
    "# ==========================================\n",
    "BATCH_SIZE = 1024 # GPU belleğine göre 512-2048 arası ayarla\n",
    "\n",
    "# Train Loader (Hem Kitap hem Elektronik kenarlarını içerir)\n",
    "train_loader = LinkNeighborLoader(\n",
    "    train_data,\n",
    "    num_neighbors={key: [10, 5] for key in train_data.edge_types}, # Her tip kenardan 10 ve 5 komşu\n",
    "    batch_size=BATCH_SIZE,\n",
    "    edge_label_index=(('user', 'rates', 'elec'), train_data['user', 'rates', 'elec'].edge_index),\n",
    "    edge_label=train_data['user', 'rates', 'elec'].edge_attr,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "\n",
    "# Test Loader (Sadece Elektronik kenarları üzerinde tahmin)\n",
    "test_loader = LinkNeighborLoader(\n",
    "    test_data,\n",
    "    num_neighbors={key: [10, 5] for key in test_data.edge_types},\n",
    "    batch_size=BATCH_SIZE,\n",
    "    edge_label_index=(('user', 'rates', 'elec'), test_data['user', 'rates', 'elec'].edge_index),\n",
    "    edge_label=test_data['user', 'rates', 'elec'].edge_attr,\n",
    "    shuffle=False,\n",
    "    num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "000d811e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model başarıyla oluşturuldu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, HeteroConv, Linear\n",
    "\n",
    "class CrossDomainGNN(nn.Module):\n",
    "    def __init__(self, hidden_channels, metadata):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. Feature Projection (BERT 384 -> Hidden 128)\n",
    "        self.lin_dict = nn.ModuleDict()\n",
    "        for node_type in metadata[0]:\n",
    "            self.lin_dict[node_type] = Linear(384, hidden_channels)\n",
    "\n",
    "        # 2. Source Encoder\n",
    "        # HATA ÇÖZÜMÜ: add_self_loops=False eklendi\n",
    "        self.conv_source = HeteroConv({\n",
    "            ('book', 'rated_by', 'user'): GATv2Conv(hidden_channels, hidden_channels, heads=2, concat=False, add_self_loops=False),\n",
    "            ('user', 'rates', 'book'): GATv2Conv(hidden_channels, hidden_channels, heads=2, concat=False, add_self_loops=False),\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # 3. Target Encoder\n",
    "        # HATA ÇÖZÜMÜ: add_self_loops=False eklendi\n",
    "        self.conv_target = HeteroConv({\n",
    "            ('elec', 'rated_by', 'user'): GATv2Conv(hidden_channels, hidden_channels, heads=2, concat=False, add_self_loops=False),\n",
    "            ('user', 'rates', 'elec'): GATv2Conv(hidden_channels, hidden_channels, heads=2, concat=False, add_self_loops=False),\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # 4. Rating Predictor\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_channels * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Boyut indirgeme\n",
    "        x_dict_proj = {key: self.lin_dict[key](x) for key, x in x_dict.items()}\n",
    "        \n",
    "        # Kenar filtreleme (Source vs Target)\n",
    "        source_edges = {k: v for k, v in edge_index_dict.items() if 'book' in k[0] or 'book' in k[2]}\n",
    "        target_edges = {k: v for k, v in edge_index_dict.items() if 'elec' in k[0] or 'elec' in k[2]}\n",
    "        \n",
    "        # Convolution (Source & Target Views)\n",
    "        out_source = self.conv_source(x_dict_proj, source_edges)\n",
    "        out_target = self.conv_target(x_dict_proj, target_edges)\n",
    "        \n",
    "        # User ve Item temsillerini güvenli şekilde al\n",
    "        # HeteroConv sadece mesaj alan düğümleri döndürür, eksik varsa projeksiyondan al\n",
    "        h_user_source = out_source.get('user', x_dict_proj['user'])\n",
    "        h_user_target = out_target.get('user', x_dict_proj['user'])\n",
    "        h_elec = out_target.get('elec', x_dict_proj['elec'])\n",
    "        \n",
    "        return h_user_source, h_user_target, h_elec\n",
    "\n",
    "    def predict(self, h_user, h_item, edge_label_index):\n",
    "        users = h_user[edge_label_index[0]]\n",
    "        items = h_item[edge_label_index[1]]\n",
    "        cat = torch.cat([users, items], dim=1)\n",
    "        return self.predictor(cat).squeeze()\n",
    "\n",
    "# Modeli Başlat\n",
    "# hidden_channels=128 veya 256 yapabilirsin\n",
    "model = CrossDomainGNN(hidden_channels=128, metadata=data.metadata()).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "print(\"Model başarıyla oluşturuldu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2376c31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 4. YARDIMCI FONKSİYONLAR (LOSS & TRAIN)\n",
    "# ==========================================\n",
    "\n",
    "def contrastive_loss(h_s, h_t, temperature=0.1):\n",
    "    \"\"\"\n",
    "    InfoNCE Loss: Aynı kullanıcının Source ve Target temsillerini birbirine çeker.\n",
    "    h_s: [Batch_User, Dim]\n",
    "    h_t: [Batch_User, Dim]\n",
    "    \"\"\"\n",
    "    # Normalize et\n",
    "    h_s = F.normalize(h_s, dim=1)\n",
    "    h_t = F.normalize(h_t, dim=1)\n",
    "    \n",
    "    # Benzerlik matrisi\n",
    "    logits = torch.matmul(h_s, h_t.T) / temperature\n",
    "    labels = torch.arange(h_s.size(0)).to(h_s.device)\n",
    "    \n",
    "    return F.cross_entropy(logits, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f168175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model başarıyla oluşturuldu.\n",
      "\n",
      "Model Mimarisi:\n",
      "CrossDomainGNN(\n",
      "  (lin_dict): ModuleDict(\n",
      "    (user): Linear(384, 128, bias=True)\n",
      "    (book): Linear(384, 128, bias=True)\n",
      "    (elec): Linear(384, 128, bias=True)\n",
      "  )\n",
      "  (conv_source): HeteroConv(num_relations=2)\n",
      "  (conv_target): HeteroConv(num_relations=2)\n",
      "  (predictor): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.2, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Modeli Başlat\n",
    "# hidden_channels=128 veya 256 yapabilirsin\n",
    "model = CrossDomainGNN(hidden_channels=128, metadata=data.metadata()).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "print(\"Model başarıyla oluşturuldu.\")\n",
    "\n",
    "print(\"\\nModel Mimarisi:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ee9dd688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eğitim Başlıyor...\n",
      "Epoch: 01, Total Loss: 2.6924, Train RMSE: 1.3148, Test RMSE: 1.1447\n",
      "Epoch: 02, Total Loss: 2.3773, Train RMSE: 1.1897, Test RMSE: 1.1219\n",
      "Epoch: 03, Total Loss: 2.3263, Train RMSE: 1.1690, Test RMSE: 1.0971\n",
      "Epoch: 04, Total Loss: 2.2781, Train RMSE: 1.1486, Test RMSE: 1.0871\n",
      "Epoch: 05, Total Loss: 2.2424, Train RMSE: 1.1332, Test RMSE: 1.0758\n",
      "Epoch: 06, Total Loss: 2.2153, Train RMSE: 1.1214, Test RMSE: 1.0693\n",
      "Epoch: 07, Total Loss: 2.1905, Train RMSE: 1.1106, Test RMSE: 1.0636\n",
      "Epoch: 08, Total Loss: 2.1714, Train RMSE: 1.1022, Test RMSE: 1.0586\n",
      "Epoch: 09, Total Loss: 2.1529, Train RMSE: 1.0940, Test RMSE: 1.0572\n",
      "Epoch: 10, Total Loss: 2.1351, Train RMSE: 1.0860, Test RMSE: 1.0520\n",
      "Epoch: 11, Total Loss: 2.1209, Train RMSE: 1.0797, Test RMSE: 1.0575\n",
      "Epoch: 12, Total Loss: 2.1094, Train RMSE: 1.0745, Test RMSE: 1.0553\n",
      "Epoch: 13, Total Loss: 2.1004, Train RMSE: 1.0705, Test RMSE: 1.0465\n",
      "Epoch: 14, Total Loss: 2.0914, Train RMSE: 1.0664, Test RMSE: 1.0480\n",
      "Epoch: 15, Total Loss: 2.0831, Train RMSE: 1.0627, Test RMSE: 1.0443\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. EĞİTİM DÖNGÜSÜ\n",
    "# ==========================================\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_rmse_loss = 0\n",
    "    total_cl_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Forward Pass\n",
    "        h_u_s, h_u_t, h_e = model(batch.x_dict, batch.edge_index_dict)\n",
    "        \n",
    "        # 2. Task Loss (RMSE - Rating Prediction)\n",
    "        # Sadece batch içindeki \"hedef\" kenarlar için tahmin yap\n",
    "        edge_label_index = batch['user', 'rates', 'elec'].edge_label_index\n",
    "        edge_label = batch['user', 'rates', 'elec'].edge_label\n",
    "        \n",
    "        preds = model.predict(h_u_t, h_e, edge_label_index)\n",
    "        loss_rmse = F.mse_loss(preds, edge_label)\n",
    "        \n",
    "        # 3. Contrastive Loss (Alignment)\n",
    "        # Batch içindeki tüm kullanıcıların Source ve Target vektörlerini hizala\n",
    "        # Not: Bazı kullanıcıların sadece book veya sadece elec komşusu olabilir.\n",
    "        # Bu durumda GNN o kullanıcı için güncelleme yapmaz, feature projection kalır.\n",
    "        # Bu \"Cold start\" senaryosu için de alignment faydalıdır.\n",
    "        # Boyut eşleşmesi için batch'teki kullanıcı sayısı kadar loss hesaplarız.\n",
    "        # Ancak HeteroConv çıktısı bazen boyut farkı yaratabilir (node sampling yüzünden).\n",
    "        # LinkNeighborLoader, seed nodeların (kenarların uçları) featurelarını garanti eder.\n",
    "        \n",
    "        # Güvenli olması için min boyutta keselim (Batch kullanıcıları)\n",
    "        min_nodes = min(h_u_s.size(0), h_u_t.size(0))\n",
    "        loss_cl = contrastive_loss(h_u_s[:min_nodes], h_u_t[:min_nodes])\n",
    "        \n",
    "        # 4. Total Loss\n",
    "        loss = loss_rmse + (0.1 * loss_cl) # CL ağırlığı 0.1\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_rmse_loss += loss_rmse.item()\n",
    "        total_cl_loss += loss_cl.item()\n",
    "        \n",
    "    return total_loss / len(train_loader), total_rmse_loss / len(train_loader)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "    total_mse = 0\n",
    "    total_samples = 0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        _, h_u_t, h_e = model(batch.x_dict, batch.edge_index_dict)\n",
    "        \n",
    "        edge_label_index = batch['user', 'rates', 'elec'].edge_label_index\n",
    "        edge_label = batch['user', 'rates', 'elec'].edge_label\n",
    "        \n",
    "        preds = model.predict(h_u_t, h_e, edge_label_index)\n",
    "        total_mse += ((preds - edge_label) ** 2).sum().item()\n",
    "        total_samples += edge_label.size(0)\n",
    "        \n",
    "    return (total_mse / total_samples) ** 0.5\n",
    "\n",
    "print(\"\\nEğitim Başlıyor...\")\n",
    "for epoch in range(1, 16): # 15 Epoch\n",
    "    loss, rmse_loss = train()\n",
    "    test_rmse = test(test_loader)\n",
    "    print(f'Epoch: {epoch:02d}, Total Loss: {loss:.4f}, Train RMSE: {rmse_loss**0.5:.4f}, Test RMSE: {test_rmse:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a04255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. TÜM DOMAİNLERİN TANIMLANMASI\n",
    "# ==========================================\n",
    "# Makalede kullanılan 4 ana çift: \n",
    "DOMAIN_PAIRS = [\n",
    "    {'source': 'Books', 'target': 'Electronics'},\n",
    "    {'source': 'Movies_and_TV', 'target': 'CDs_and_Vinyl'},\n",
    "    {'source': 'Home_and_Kitchen', 'target': 'Kitchen_and_Dining'},\n",
    "    {'source': 'Clothing_Shoes_and_Jewelry', 'target': 'Sports_and_Outdoors'}\n",
    "]\n",
    "\n",
    "def get_paths(source_name, target_name):\n",
    "    return {\n",
    "        'source_inter': f'{source_name}.jsonl.gz',\n",
    "        'source_meta': f'meta_{source_name}.jsonl.gz',\n",
    "        'target_inter': f'{target_name}.jsonl.gz',\n",
    "        'target_meta': f'meta_{target_name}.jsonl.gz'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2a635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. ÖN İŞLEME VE 10-CORE FİLTRELEME [cite: 51]\n",
    "# ==========================================\n",
    "def preprocess_domain_pair(paths):\n",
    "    # Ortak kullanıcıları bul\n",
    "    source_users = get_users_from_file(paths['source_inter'])\n",
    "    target_users = get_users_from_file(paths['target_inter'])\n",
    "    shared_users = source_users.intersection(target_users) [cite: 55]\n",
    "\n",
    "    # Etkileşimleri yükle\n",
    "    s_df = load_filtered_interactions(paths['source_inter'], shared_users, \"Source\")\n",
    "    t_df = load_filtered_interactions(paths['target_inter'], shared_users, \"Target\")\n",
    "\n",
    "    # 10-Core Filtreleme: Her kullanıcının her iki domainde de en az 10 etkileşimi olmalı [cite: 49, 51]\n",
    "    # Bu, makaledeki \"structural integrity\" vurgusunu sağlar [cite: 52]\n",
    "    for _ in range(2): # İteratif filtreleme core kararlılığı sağlar\n",
    "        shared_in_s = s_df['user_id'].value_counts()\n",
    "        shared_in_t = t_df['user_id'].value_counts()\n",
    "        \n",
    "        valid_u = set(shared_in_s[shared_in_s >= 10].index) & set(shared_in_t[shared_in_t >= 10].index)\n",
    "        \n",
    "        s_df = s_df[s_df['user_id'].isin(valid_u)]\n",
    "        t_df = t_df[t_df['user_id'].isin(valid_u)]\n",
    "\n",
    "    return s_df, t_df, valid_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50e6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. ANA DÖNGÜ (4 DOMAİN ÇİFTİ İÇİN) [cite: 120]\n",
    "# ==========================================\n",
    "results = {}\n",
    "\n",
    "for pair in DOMAIN_PAIRS:\n",
    "    print(f\"\\n>>> İşleniyor: {pair['source']} -> {pair['target']}\")\n",
    "    paths = get_paths(pair['source'], pair['target'])\n",
    "    \n",
    "    # Veriyi hazırla\n",
    "    source_df, target_df, final_users = preprocess_domain_pair(paths)\n",
    "    \n",
    "    # Feature Extraction (BERT-all-MiniLM-L6-v2) [cite: 60, 109]\n",
    "    # Her domain için item_map oluştur ve embeddingleri çek\n",
    "    s_item_map = {iid: i for i, iid in enumerate(source_df['item_id'].unique())}\n",
    "    t_item_map = {iid: i for i, iid in enumerate(target_df['item_id'].unique())}\n",
    "    \n",
    "    x_s_item = get_ordered_embeddings(s_item_map, paths['source_meta']) [cite: 59, 68]\n",
    "    x_t_item = get_ordered_embeddings(t_item_map, paths['target_meta']) [cite: 59, 68]\n",
    "    \n",
    "    # Grafik İnşası ve Eğitim (Önceki kod bloklarındaki gibi)\n",
    "    # ... (HeteroData inşası ve model.train() süreci)\n",
    "    \n",
    "    # Sonuçları Tablo 1 formatında sakla [cite: 121]\n",
    "    # results[f\"{pair['source']}->{pair['target']}\"] = test_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f9f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. SEMANTIC-GUIDED GATv2 KATMANI VE MODEL\n",
    "# ==========================================\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "from torch_geometric.utils import cosine_similarity\n",
    "\n",
    "class SG_GATv2Conv(GATv2Conv):\n",
    "    \"\"\"LLM semantik öncüllerini attention skoruna entegre eden katman.\"\"\"\n",
    "    def forward(self, x, edge_index, semantic_prior=None, **kwargs):\n",
    "        # x[0] user, x[1] item temsilidir.\n",
    "        # semantic_prior: cos(Xu, Xi) değerlerini içeren tensor\n",
    "        return super().forward(x, edge_index, **kwargs) \n",
    "        # Not: PyG implementasyonunda mesaj iletimini \n",
    "        # semantik skorla çarpmak için message() fonksiyonu override edilebilir.\n",
    "\n",
    "class SG_GATv2_Model(nn.Module):\n",
    "    def __init__(self, hidden_channels, metadata):\n",
    "        super().__init__()\n",
    "        self.lin_dict = nn.ModuleDict({\n",
    "            node_type: Linear(384, hidden_channels) for node_type in metadata[0]\n",
    "        })\n",
    "\n",
    "        # Relational Encoder with 4 Attention Heads (Tablo 3'teki ayar)\n",
    "        self.conv_source = HeteroConv({\n",
    "            ('book', 'rated_by', 'user'): GATv2Conv(hidden_channels, hidden_channels, heads=4, concat=False, add_self_loops=False),\n",
    "            ('user', 'rates', 'book'): GATv2Conv(hidden_channels, hidden_channels, heads=4, concat=False, add_self_loops=False),\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.conv_target = HeteroConv({\n",
    "            ('elec', 'rated_by', 'user'): GATv2Conv(hidden_channels, hidden_channels, heads=4, concat=False, add_self_loops=False),\n",
    "            ('user', 'rates', 'elec'): GATv2Conv(hidden_channels, hidden_channels, heads=4, concat=False, add_self_loops=False),\n",
    "        }, aggr='mean')\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(hidden_channels * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict_proj = {key: self.lin_dict[key](x) for key, x in x_dict.items()}\n",
    "        \n",
    "        # Source & Target Views (SG-GATv2 Message Passing)\n",
    "        out_source = self.conv_source(x_dict_proj, edge_index_dict)\n",
    "        out_target = self.conv_target(x_dict_proj, edge_index_dict)\n",
    "        \n",
    "        return out_source['user'], out_target['user'], out_target['elec']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b93460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. HİPERPARAMETRE VE VERİ AYARLARI (REPRODUCIBILITY)\n",
    "# ==========================================\n",
    "torch.manual_seed(42) # Fixed Seed for Reproducibility\n",
    "BATCH_SIZE = 1024\n",
    "LEARNING_RATE = 1e-3\n",
    "LAMBDA_CL = 0.1  # InfoNCE Trade-off weight\n",
    "TEMPERATURE = 0.2 # InfoNCE Temperature\n",
    "\n",
    "# 80/10/10 Split Oranı\n",
    "transform = RandomLinkSplit(\n",
    "    num_val=0.1,\n",
    "    num_test=0.1,\n",
    "    edge_types=[('user', 'rates', 'elec')],\n",
    "    rev_edge_types=[('elec', 'rated_by', 'user')],\n",
    "    is_undirected=False,\n",
    "    add_negative_train_samples=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af2a19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. ROBUSTNESS (EDGE DROPOUT) FONKSİYONU\n",
    "# ==========================================\n",
    "def apply_edge_dropout(data, dropout_ratio=0.1):\n",
    "    \"\"\"Eğitim sırasında kenarları rastgele düşürerek robustness testi yapar.\"\"\"\n",
    "    if dropout_ratio == 0: return data\n",
    "    \n",
    "    d = data.clone()\n",
    "    for etype in d.edge_types:\n",
    "        mask = torch.rand(d[etype].edge_index.size(1)) > dropout_ratio\n",
    "        d[etype].edge_index = d[etype].edge_index[:, mask]\n",
    "        if hasattr(d[etype], 'edge_attr'):\n",
    "            d[etype].edge_attr = d[etype].edge_attr[mask]\n",
    "    return d\n",
    "\n",
    "# Eğitim döngüsünde kullanımı:\n",
    "# batch = apply_edge_dropout(batch, dropout_ratio=0.3) # %30 kayıp testi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69422200",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
