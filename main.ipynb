{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa6e53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "from collections import Counter\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c888fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_shared_users(source_path, target_path):\n",
    "#     print(\"Kaynak domain kullanıcıları taranıyor...\")\n",
    "#     source_users = set()\n",
    "#     with gzip.open(source_path, 'rt', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             data = json.loads(line)\n",
    "#             source_users.add(data['user_id'])\n",
    "            \n",
    "#     print(\"Hedef domain kullanıcıları ve çakışmalar taranıyor...\")\n",
    "#     shared_users = set()\n",
    "#     with gzip.open(target_path, 'rt', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             data = json.loads(line)\n",
    "#             if data['user_id'] in source_users:\n",
    "#                 shared_users.add(data['user_id'])\n",
    "    \n",
    "#     print(f\"Toplam Ortak Kullanıcı Sayısı: {len(shared_users)}\")\n",
    "#     return shared_users\n",
    "\n",
    "# # Dosya yollarını güncelleyin\n",
    "# source_domain_file = \"Books.jsonl.gz\"\n",
    "# target_domain_file = \"Electronics.jsonl.gz\"\n",
    "\n",
    "# shared_users = get_shared_users(source_domain_file, target_domain_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5da50ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def filter_and_save_data(path, shared_users, output_name, min_inter=20): # min_iter=5\n",
    "#     extracted_data = []\n",
    "#     user_counts = Counter()\n",
    "    \n",
    "#     print(f\"{output_name} işleniyor...\")\n",
    "#     with gzip.open(path, 'rt', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             data = json.loads(line)\n",
    "#             # Sadece ortak kullanıcıları veya hedef domaindeki etkileşimleri al\n",
    "#             if data['user_id'] in shared_users:\n",
    "#                 extracted_data.append({\n",
    "#                     'user_id': data['user_id'],\n",
    "#                     'item_id': data['parent_asin'], # Amazon 2023'te parent_asin kullanımı önerilir\n",
    "#                     'rating': data['rating'],\n",
    "#                     'timestamp': data['timestamp']\n",
    "#                 })\n",
    "#                 user_counts[data['user_id']] += 1\n",
    "                \n",
    "#     df = pd.DataFrame(extracted_data)\n",
    "    \n",
    "#     # 5-core filtering: En az 5 yorumu olan kullanıcıları tut (Akademik standart)\n",
    "#     df = df[df.groupby('user_id')['user_id'].transform('count') >= min_inter]\n",
    "    \n",
    "#     df.to_csv(f\"{output_name}_filtered.csv\", index=False)\n",
    "#     print(f\"{output_name} kaydedildi. Satır sayısı: {len(df)}\")\n",
    "\n",
    "# # Kullanım:\n",
    "# filter_and_save_data(source_domain_file, shared_users, \"source_books\")\n",
    "# filter_and_save_data(target_domain_file, shared_users, \"target_electronics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca1339b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- source_books_filtered.csv Analizi ---\n",
      "Kullanıcı: 88749 | Ürün: 1459222 | Etkileşim: 4287071\n",
      "Sparsity: %99.9967\n",
      "\n",
      "--- target_electronics_filtered.csv Analizi ---\n",
      "Kullanıcı: 80650 | Ürün: 489371 | Etkileşim: 2801916\n",
      "Sparsity: %99.9929\n"
     ]
    }
   ],
   "source": [
    "def analyze_sparsity(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    u = df['user_id'].nunique()\n",
    "    i = df['item_id'].nunique()\n",
    "    r = len(df)\n",
    "    sparsity = (1 - (r / (u * i))) * 100\n",
    "    print(f\"\\n--- {csv_path} Analizi ---\")\n",
    "    print(f\"Kullanıcı: {u} | Ürün: {i} | Etkileşim: {r}\")\n",
    "    print(f\"Sparsity: %{sparsity:.4f}\")\n",
    "\n",
    "analyze_sparsity(\"source_books_filtered.csv\")\n",
    "analyze_sparsity(\"target_electronics_filtered.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dcc229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, HeteroConv\n",
    "from torch.autograd import Function\n",
    "\n",
    "# 1. Gradient Reversal Layer (GRL) - Adversarial Learning için\n",
    "class GRL(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # Geriye yayılımda gradyanı ters çevirir (-alpha ile çarpar)\n",
    "        return grad_output.neg() * ctx.alpha, None\n",
    "\n",
    "# 2. GAT-Based Encoder (Niyet Yakalayıcı)\n",
    "class DualIntentEncoder(nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, heads=4):\n",
    "        super().__init__()\n",
    "        # Katman 1: Çok kafalı dikkat (Multi-head attention)\n",
    "        self.conv1 = HeteroConv({\n",
    "            ('user', 'interacts', 'item'): GATConv((-1, -1), hidden_channels, heads=heads, add_self_loops=False),\n",
    "            ('item', 'rev_interacts', 'user'): GATConv((-1, -1), hidden_channels, heads=heads, add_self_loops=False),\n",
    "        }, aggr='mean')\n",
    "        \n",
    "        # Katman 2: Çıktı katmanı\n",
    "        self.conv2 = HeteroConv({\n",
    "            ('user', 'interacts', 'item'): GATConv((-1 * heads, -1 * heads), out_channels, heads=1, add_self_loops=False),\n",
    "            ('item', 'rev_interacts', 'user'): GATConv((-1 * heads, -1 * heads), out_channels, heads=1, add_self_loops=False),\n",
    "        }, aggr='mean')\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.elu(x) for key, x in x_dict.items()}\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        return x_dict # {'user': [N, out_dim], 'item': [M, out_dim]}\n",
    "\n",
    "# 3. Domain Discriminator (Domain Bağımsız Özellik Öğrenme)\n",
    "class DomainDiscriminator(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(emb_dim, 64),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, alpha):\n",
    "        x = GRL.apply(x, alpha)\n",
    "        return self.net(x)\n",
    "\n",
    "# 4. Contrastive Alignment Loss (InfoNCE)\n",
    "def info_nce_loss(z_source, z_target, shared_users, temperature=0.1):\n",
    "    \"\"\"\n",
    "    Shared users üzerinden iki domaini hizalar.\n",
    "    z_source/target: Tüm user embeddingleri\n",
    "    shared_users: [shared_count, 2] -> [[source_idx, target_idx], ...]\n",
    "    \"\"\"\n",
    "    z_s = z_source[shared_users[:, 0]]\n",
    "    z_t = z_target[shared_users[:, 1]]\n",
    "    \n",
    "    z_s = F.normalize(z_s, dim=1)\n",
    "    z_t = F.normalize(z_t, dim=1)\n",
    "    \n",
    "    # Benzerlik matrisi (N x N)\n",
    "    logits = torch.matmul(z_s, z_t.T) / temperature\n",
    "    labels = torch.arange(z_s.size(0)).to(z_s.device)\n",
    "    \n",
    "    # Simetrik Loss (S -> T ve T -> S)\n",
    "    loss_i2j = F.cross_entropy(logits, labels)\n",
    "    loss_j2i = F.cross_entropy(logits.T, labels)\n",
    "    \n",
    "    return (loss_i2j + loss_j2i) / 2\n",
    "\n",
    "# 5. Model Birleştirici (Main Wrapper)\n",
    "class CDRModel(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = DualIntentEncoder(32, emb_dim)\n",
    "        self.discriminator = DomainDiscriminator(emb_dim)\n",
    "        # Rating tahmini için basit MLP\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(emb_dim * 2, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def predict(self, user_emb, item_emb, edges):\n",
    "        u_idx, i_idx = edges[0], edges[1]\n",
    "        cat_feat = torch.cat([user_emb[u_idx], item_emb[i_idx]], dim=1)\n",
    "        return self.predictor(cat_feat).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdccf81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veriler yükleniyor...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Filtrelediğimiz dosyaları geri yüklüyoruz\n",
    "print(\"Veriler yükleniyor...\")\n",
    "books_df = pd.read_csv(\"source_books_filtered.csv\")\n",
    "elec_df = pd.read_csv(\"target_electronics_filtered.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f12f8294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eşleşme tamamlandı. Ortak kullanıcı tensör boyutu: torch.Size([7648, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "def create_shared_user_mapping(source_df, target_df):\n",
    "    # Her iki domaindeki benzersiz kullanıcıları al\n",
    "    source_users = source_df['user_id'].unique()\n",
    "    target_users = target_df['user_id'].unique()\n",
    "    \n",
    "    # ID -> Index eşlemesi oluştur\n",
    "    s_user2idx = {id: i for i, id in enumerate(source_users)}\n",
    "    t_user2idx = {id: i for i, id in enumerate(target_users)}\n",
    "    \n",
    "    # Ortak kullanıcıları bul ve index çiftlerini oluştur\n",
    "    shared_ids = set(source_users).intersection(set(target_users))\n",
    "    shared_mapping = []\n",
    "    for uid in shared_ids:\n",
    "        shared_mapping.append([s_user2idx[uid], t_user2idx[uid]])\n",
    "    \n",
    "    return torch.tensor(shared_mapping, dtype=torch.long), s_user2idx, t_user2idx\n",
    "\n",
    "# Kullanım:\n",
    "# Şimdi hata aldığınız fonksiyonu çağırabilirsiniz\n",
    "shared_idx_tensor, s_map, t_map = create_shared_user_mapping(books_df, elec_df)\n",
    "print(f\"Eşleşme tamamlandı. Ortak kullanıcı tensör boyutu: {shared_idx_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6cd72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "def build_graph(df, user_map, item_name='item'):\n",
    "    data = HeteroData()\n",
    "    \n",
    "    # Ürün ID'lerini indexle\n",
    "    items = df['item_id'].unique()\n",
    "    item_map = {id: i for i, id in enumerate(items)}\n",
    "    \n",
    "    # Kenarları (Edges) oluştur\n",
    "    u_idx = [user_map[uid] for uid in df['user_id']]\n",
    "    i_idx = [item_map[iid] for iid in df['item_id']]\n",
    "    \n",
    "    edge_index = torch.tensor([u_idx, i_idx], dtype=torch.long)\n",
    "    edge_attr = torch.tensor(df['rating'].values, dtype=torch.float)\n",
    "    \n",
    "    # Düğüm özelliklerini (features) basitçe ilklendir (Q1 için burası daha sonra zenginleştirilecek)\n",
    "    data['user'].x = torch.randn(len(user_map), 16)\n",
    "    data['item'].x = torch.randn(len(item_map), 16)\n",
    "    \n",
    "    data['user', 'interacts', 'item'].edge_index = edge_index\n",
    "    data['user', 'interacts', 'item'].edge_attr = edge_attr\n",
    "    \n",
    "    # Ters kenarları ekle (GNN mesaj iletimi için şart)\n",
    "    data['item', 'rev_interacts', 'user'].edge_index = edge_index[[1, 0]]\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Grafikleri oluştur\n",
    "books_graph = build_graph(books_df, s_map)\n",
    "elec_graph = build_graph(elec_df, t_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09fcb185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veriler CSV'den yükleniyor...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# 1. Veriyi Hazırlama (Önceki adımda hata aldığınız yerin düzeltilmiş hali)\n",
    "print(\"Veriler CSV'den yükleniyor...\")\n",
    "books_df = pd.read_csv(\"source_books_filtered.csv\")\n",
    "elec_df = pd.read_csv(\"target_electronics_filtered.csv\")\n",
    "\n",
    "# Ortak kullanıcı haritasını oluştur\n",
    "shared_idx_tensor, s_map, t_map = create_shared_user_mapping(books_df, elec_df)\n",
    "\n",
    "# Grafikleri oluştur (Daha önce verdiğim build_graph fonksiyonunu kullanın)\n",
    "books_graph = build_graph(books_df, s_map)\n",
    "elec_graph = build_graph(elec_df, t_map)\n",
    "\n",
    "# 2. NeighborLoader: Büyük veri için hayati önem taşıyan örnekleyici\n",
    "# Her batch'te 1024 kullanıcı ve onların 2 katmanlı komşuları (15 ve 10 komşu) alınır.\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "# Books (Kaynak) için\n",
    "train_loader_s = LinkNeighborLoader(\n",
    "    books_graph,\n",
    "    num_neighbors=[10, 5], \n",
    "    batch_size=2048, # Batch size'ı artırmak GPU paralelliğini artırır, süreyi kısaltır\n",
    "    edge_label_index=(('user', 'interacts', 'item'), books_graph['user', 'interacts', 'item'].edge_index),\n",
    "    edge_label=books_graph['user', 'interacts', 'item'].edge_attr,\n",
    "    shuffle=True,\n",
    "    num_workers=4, # CPU çekirdeklerini kullan\n",
    "    persistent_workers=True # Her batch'te worker'ları yeniden başlatma\n",
    ")\n",
    "# train_loader_s = LinkNeighborLoader(\n",
    "#     books_graph,\n",
    "#     num_neighbors=[10, 5], # Büyük veri için [15, 10] yerine daha hızlı [10, 5] öneririm\n",
    "#     batch_size=1024,\n",
    "#     edge_label_index=(('user', 'interacts', 'item'), books_graph['user', 'interacts', 'item'].edge_index),\n",
    "#     edge_label=books_graph['user', 'interacts', 'item'].edge_attr,\n",
    "#     shuffle=True,\n",
    "#     # Önemli: 'neighbor_sampler' kütüphane hatasını aşmak için:\n",
    "#     subgraph_type='induced' \n",
    "# )\n",
    "\n",
    "# Electronics (Hedef) için\n",
    "train_loader_t = LinkNeighborLoader(\n",
    "    elec_graph,\n",
    "    num_neighbors=[10, 5],\n",
    "    batch_size=2048, # Süreyi kısaltmak için batch size artırıldı\n",
    "    edge_label_index=(('user', 'interacts', 'item'), elec_graph['user', 'interacts', 'item'].edge_index),\n",
    "    # EKSİK OLAN SATIR BURASIYDI:\n",
    "    edge_label=elec_graph['user', 'interacts', 'item'].edge_attr, \n",
    "    shuffle=False, # Değerlendirme yaparken karıştırmaya gerek yok\n",
    "    subgraph_type='induced'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c107a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38ce5300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eğitim cuda üzerinde başlıyor...\n",
      "Batch 0 | Loss: 18.8296\n",
      "Batch 100 | Loss: 1.3756\n",
      "Batch 200 | Loss: 1.1455\n",
      "Batch 300 | Loss: 1.0364\n",
      "Batch 400 | Loss: 1.0725\n",
      "Batch 500 | Loss: 1.0161\n",
      "Batch 600 | Loss: 1.0210\n",
      "Batch 700 | Loss: 0.9843\n",
      "Batch 800 | Loss: 1.0222\n",
      "Batch 900 | Loss: 1.0674\n",
      "Batch 1000 | Loss: 0.9667\n",
      "Batch 1100 | Loss: 1.0292\n",
      "Batch 1200 | Loss: 0.9339\n",
      "Batch 1300 | Loss: 0.9714\n",
      "Batch 1400 | Loss: 1.0597\n",
      "Batch 1500 | Loss: 1.0099\n",
      "Batch 1600 | Loss: 1.0082\n",
      "Batch 1700 | Loss: 1.0448\n",
      "Batch 1800 | Loss: 1.1163\n",
      "Batch 1900 | Loss: 0.9656\n",
      "Batch 2000 | Loss: 1.0232\n",
      "Epoch: 01, Loss: 1.2056\n",
      "Batch 0 | Loss: 1.0502\n",
      "Batch 100 | Loss: 1.0887\n",
      "Batch 200 | Loss: 1.0484\n",
      "Batch 300 | Loss: 1.0881\n",
      "Batch 400 | Loss: 0.9572\n",
      "Batch 500 | Loss: 1.0143\n",
      "Batch 600 | Loss: 1.1152\n",
      "Batch 700 | Loss: 1.0058\n",
      "Batch 800 | Loss: 1.1202\n",
      "Batch 900 | Loss: 1.0685\n",
      "Batch 1000 | Loss: 1.0590\n",
      "Batch 1100 | Loss: 1.0345\n",
      "Batch 1200 | Loss: 1.0157\n",
      "Batch 1300 | Loss: 1.0140\n",
      "Batch 1400 | Loss: 0.9649\n",
      "Batch 1500 | Loss: 0.9275\n",
      "Batch 1600 | Loss: 1.0189\n",
      "Batch 1700 | Loss: 0.9928\n",
      "Batch 1800 | Loss: 1.0355\n",
      "Batch 1900 | Loss: 1.0193\n",
      "Batch 2000 | Loss: 1.0654\n",
      "Epoch: 02, Loss: 1.0352\n",
      "Batch 0 | Loss: 1.0896\n",
      "Batch 100 | Loss: 0.9855\n",
      "Batch 200 | Loss: 1.0603\n",
      "Batch 300 | Loss: 1.0550\n",
      "Batch 400 | Loss: 1.0618\n",
      "Batch 500 | Loss: 1.0929\n",
      "Batch 600 | Loss: 1.1029\n",
      "Batch 700 | Loss: 1.0224\n",
      "Batch 800 | Loss: 1.0530\n",
      "Batch 900 | Loss: 1.0648\n",
      "Batch 1000 | Loss: 1.0317\n",
      "Batch 1100 | Loss: 0.9653\n",
      "Batch 1200 | Loss: 0.9815\n",
      "Batch 1300 | Loss: 1.1670\n",
      "Batch 1400 | Loss: 1.0580\n",
      "Batch 1500 | Loss: 0.9784\n",
      "Batch 1600 | Loss: 1.0014\n",
      "Batch 1700 | Loss: 0.9850\n",
      "Batch 1800 | Loss: 0.9721\n",
      "Batch 1900 | Loss: 1.0651\n",
      "Batch 2000 | Loss: 0.9826\n",
      "Epoch: 03, Loss: 1.0323\n",
      "Batch 0 | Loss: 1.0520\n",
      "Batch 100 | Loss: 1.0375\n",
      "Batch 200 | Loss: 1.0152\n",
      "Batch 300 | Loss: 1.0803\n",
      "Batch 400 | Loss: 0.9811\n",
      "Batch 500 | Loss: 0.9685\n",
      "Batch 600 | Loss: 1.0060\n",
      "Batch 700 | Loss: 1.0520\n",
      "Batch 800 | Loss: 1.0214\n",
      "Batch 900 | Loss: 1.0449\n",
      "Batch 1000 | Loss: 1.0213\n",
      "Batch 1100 | Loss: 1.0299\n",
      "Batch 1200 | Loss: 0.9426\n",
      "Batch 1300 | Loss: 1.0047\n",
      "Batch 1400 | Loss: 0.9590\n",
      "Batch 1500 | Loss: 1.0093\n",
      "Batch 1600 | Loss: 1.0662\n",
      "Batch 1700 | Loss: 0.9912\n",
      "Batch 1800 | Loss: 1.0270\n",
      "Batch 1900 | Loss: 1.0231\n",
      "Batch 2000 | Loss: 1.0312\n",
      "Epoch: 04, Loss: 1.0325\n",
      "Batch 0 | Loss: 1.0394\n",
      "Batch 100 | Loss: 1.0164\n",
      "Batch 200 | Loss: 1.0564\n",
      "Batch 300 | Loss: 1.0851\n",
      "Batch 400 | Loss: 0.9829\n",
      "Batch 500 | Loss: 1.0611\n",
      "Batch 600 | Loss: 1.0244\n",
      "Batch 700 | Loss: 1.0713\n",
      "Batch 800 | Loss: 1.0043\n",
      "Batch 900 | Loss: 0.9963\n",
      "Batch 1000 | Loss: 1.0117\n",
      "Batch 1100 | Loss: 1.0328\n",
      "Batch 1200 | Loss: 1.0363\n",
      "Batch 1300 | Loss: 1.0266\n",
      "Batch 1400 | Loss: 1.0177\n",
      "Batch 1500 | Loss: 0.9245\n",
      "Batch 1600 | Loss: 1.0400\n",
      "Batch 1700 | Loss: 1.0469\n",
      "Batch 1800 | Loss: 1.0421\n",
      "Batch 1900 | Loss: 1.0682\n",
      "Batch 2000 | Loss: 0.9945\n",
      "Epoch: 05, Loss: 1.0313\n",
      "Batch 0 | Loss: 1.0408\n",
      "Batch 100 | Loss: 1.1225\n",
      "Batch 200 | Loss: 1.1214\n",
      "Batch 300 | Loss: 1.0554\n",
      "Batch 400 | Loss: 0.9314\n",
      "Batch 500 | Loss: 0.9908\n",
      "Batch 600 | Loss: 0.9705\n",
      "Batch 700 | Loss: 1.0006\n",
      "Batch 800 | Loss: 1.0674\n",
      "Batch 900 | Loss: 1.0098\n",
      "Batch 1000 | Loss: 1.0222\n",
      "Batch 1100 | Loss: 1.0767\n",
      "Batch 1200 | Loss: 1.0161\n",
      "Batch 1300 | Loss: 0.9941\n",
      "Batch 1400 | Loss: 1.0085\n",
      "Batch 1500 | Loss: 1.0256\n",
      "Batch 1600 | Loss: 1.0979\n",
      "Batch 1700 | Loss: 0.9284\n",
      "Batch 1800 | Loss: 1.1122\n",
      "Batch 1900 | Loss: 1.0141\n",
      "Batch 2000 | Loss: 1.0812\n",
      "Epoch: 06, Loss: 1.0298\n",
      "Batch 0 | Loss: 0.9829\n",
      "Batch 100 | Loss: 0.9919\n",
      "Batch 200 | Loss: 0.9099\n",
      "Batch 300 | Loss: 1.0690\n",
      "Batch 400 | Loss: 1.0708\n",
      "Batch 500 | Loss: 1.0309\n",
      "Batch 600 | Loss: 1.0505\n",
      "Batch 700 | Loss: 0.9922\n",
      "Batch 800 | Loss: 0.9680\n",
      "Batch 900 | Loss: 1.0153\n",
      "Batch 1000 | Loss: 1.0678\n",
      "Batch 1100 | Loss: 1.0190\n",
      "Batch 1200 | Loss: 1.0295\n",
      "Batch 1300 | Loss: 0.9656\n",
      "Batch 1400 | Loss: 1.0263\n",
      "Batch 1500 | Loss: 1.1019\n",
      "Batch 1600 | Loss: 0.9674\n",
      "Batch 1700 | Loss: 1.0371\n",
      "Batch 1800 | Loss: 0.9870\n",
      "Batch 1900 | Loss: 1.0310\n",
      "Batch 2000 | Loss: 1.0337\n",
      "Epoch: 07, Loss: 1.0286\n",
      "Batch 0 | Loss: 1.0050\n",
      "Batch 100 | Loss: 1.0228\n",
      "Batch 200 | Loss: 0.9798\n",
      "Batch 300 | Loss: 1.0209\n",
      "Batch 400 | Loss: 0.9570\n",
      "Batch 500 | Loss: 1.0402\n",
      "Batch 600 | Loss: 1.0540\n",
      "Batch 700 | Loss: 0.9441\n",
      "Batch 800 | Loss: 1.0080\n",
      "Batch 900 | Loss: 1.1139\n",
      "Batch 1000 | Loss: 1.0348\n",
      "Batch 1100 | Loss: 0.9995\n",
      "Batch 1200 | Loss: 1.0138\n",
      "Batch 1300 | Loss: 1.0246\n",
      "Batch 1400 | Loss: 1.0565\n",
      "Batch 1500 | Loss: 1.0021\n",
      "Batch 1600 | Loss: 1.0471\n",
      "Batch 1700 | Loss: 1.0118\n",
      "Batch 1800 | Loss: 1.0250\n",
      "Batch 1900 | Loss: 0.9657\n",
      "Batch 2000 | Loss: 1.0814\n",
      "Epoch: 08, Loss: 1.0260\n",
      "Batch 0 | Loss: 1.0005\n",
      "Batch 100 | Loss: 1.0248\n",
      "Batch 200 | Loss: 1.0347\n",
      "Batch 300 | Loss: 1.0369\n",
      "Batch 400 | Loss: 1.0472\n",
      "Batch 500 | Loss: 0.9823\n",
      "Batch 600 | Loss: 1.0352\n",
      "Batch 700 | Loss: 1.0310\n",
      "Batch 800 | Loss: 1.0181\n",
      "Batch 900 | Loss: 0.9852\n",
      "Batch 1000 | Loss: 1.0349\n",
      "Batch 1100 | Loss: 0.9814\n",
      "Batch 1200 | Loss: 0.9961\n",
      "Batch 1300 | Loss: 1.0315\n",
      "Batch 1400 | Loss: 1.0475\n",
      "Batch 1500 | Loss: 1.0083\n",
      "Batch 1600 | Loss: 0.9786\n",
      "Batch 1700 | Loss: 1.0695\n",
      "Batch 1800 | Loss: 0.9895\n",
      "Batch 1900 | Loss: 0.9937\n",
      "Batch 2000 | Loss: 1.0290\n",
      "Epoch: 09, Loss: 1.0255\n",
      "Batch 0 | Loss: 1.0858\n",
      "Batch 100 | Loss: 0.9711\n",
      "Batch 200 | Loss: 1.0432\n",
      "Batch 300 | Loss: 1.0412\n",
      "Batch 400 | Loss: 0.9919\n",
      "Batch 500 | Loss: 1.1088\n",
      "Batch 600 | Loss: 1.0054\n",
      "Batch 700 | Loss: 1.0734\n",
      "Batch 800 | Loss: 1.0613\n",
      "Batch 900 | Loss: 0.9977\n",
      "Batch 1000 | Loss: 0.9918\n",
      "Batch 1100 | Loss: 1.0816\n",
      "Batch 1200 | Loss: 1.0996\n",
      "Batch 1300 | Loss: 0.9784\n",
      "Batch 1400 | Loss: 0.9246\n",
      "Batch 1500 | Loss: 1.0586\n",
      "Batch 1600 | Loss: 0.9601\n",
      "Batch 1700 | Loss: 0.9685\n",
      "Batch 1800 | Loss: 1.0486\n",
      "Batch 1900 | Loss: 1.0673\n",
      "Batch 2000 | Loss: 0.9740\n",
      "Epoch: 10, Loss: 1.0240\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CDRModel(emb_dim=64).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    alpha = 0.1 \n",
    "    \n",
    "    loader_t_iter = iter(train_loader_t)\n",
    "    \n",
    "    for i, batch_s in enumerate(train_loader_s):\n",
    "        try:\n",
    "            batch_t = next(loader_t_iter)\n",
    "        except StopIteration:\n",
    "            loader_t_iter = iter(train_loader_t)\n",
    "            batch_t = next(loader_t_iter)\n",
    "            \n",
    "        batch_s, batch_t = batch_s.to(device), batch_t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Forward Pass (Encoder)\n",
    "        z_s_dict = model.encoder(batch_s.x_dict, batch_s.edge_index_dict)\n",
    "        z_t_dict = model.encoder(batch_t.x_dict, batch_t.edge_index_dict)\n",
    "        \n",
    "        # 2. Rating Prediction Loss (Sadece Books Üzerinden)\n",
    "        # LinkNeighborLoader'da seçilen kenarlar 'edge_label_index' içindedir\n",
    "        edge_index_s = batch_s['user', 'interacts', 'item'].edge_label_index\n",
    "        labels_s = batch_s['user', 'interacts', 'item'].edge_label\n",
    "        \n",
    "        preds_s = model.predict(z_s_dict['user'], z_s_dict['item'], edge_index_s)\n",
    "        loss_task = F.mse_loss(preds_s, labels_s)\n",
    "        \n",
    "        # 3. Adversarial Loss (Domain Invariance)\n",
    "        d_s = model.discriminator(z_s_dict['user'], alpha)\n",
    "        d_t = model.discriminator(z_t_dict['user'], alpha)\n",
    "        loss_adv = F.binary_cross_entropy(d_s, torch.zeros_like(d_s)) + \\\n",
    "                   F.binary_cross_entropy(d_t, torch.ones_like(d_t))\n",
    "        \n",
    "        # Toplam Kayıp\n",
    "        loss = loss_task + 0.1 * loss_adv\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Batch {i} | Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    return total_loss / (i + 1)\n",
    "# Eğitimi Başlat\n",
    "print(f\"Eğitim {device} üzerinde başlıyor...\")\n",
    "for epoch in range(1, 11):\n",
    "    loss = train()\n",
    "    print(f\"Epoch: {epoch:02d}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "874a28f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eğitim cuda üzerinde başlıyor...\n",
      "Batch 0 | Loss: 20.1289\n",
      "Batch 100 | Loss: 1.3885\n",
      "Batch 200 | Loss: 1.1952\n",
      "Batch 300 | Loss: 1.0148\n",
      "Batch 400 | Loss: 1.0395\n",
      "Batch 500 | Loss: 1.0120\n",
      "Batch 600 | Loss: 1.0073\n",
      "Batch 700 | Loss: 0.9665\n",
      "Batch 800 | Loss: 1.0384\n",
      "Batch 900 | Loss: 0.9626\n",
      "Batch 1000 | Loss: 1.0171\n",
      "Batch 1100 | Loss: 0.9523\n",
      "Batch 1200 | Loss: 1.0086\n",
      "Batch 1300 | Loss: 0.9308\n",
      "Batch 1400 | Loss: 0.9593\n",
      "Batch 1500 | Loss: 1.0512\n",
      "Batch 1600 | Loss: 0.8925\n",
      "Batch 1700 | Loss: 0.9173\n",
      "Batch 1800 | Loss: 1.0056\n",
      "Batch 1900 | Loss: 0.9695\n",
      "Batch 2000 | Loss: 1.0445\n",
      "Epoch: 01, Loss: 1.2079\n",
      "Batch 0 | Loss: 1.0094\n",
      "Batch 100 | Loss: 1.0289\n",
      "Batch 200 | Loss: 1.0309\n",
      "Batch 300 | Loss: 1.0079\n",
      "Batch 400 | Loss: 1.0214\n",
      "Batch 500 | Loss: 1.1097\n",
      "Batch 600 | Loss: 0.9554\n",
      "Batch 700 | Loss: 1.1759\n",
      "Batch 800 | Loss: 1.0533\n",
      "Batch 900 | Loss: 1.0367\n",
      "Batch 1000 | Loss: 1.0112\n",
      "Batch 1100 | Loss: 1.0153\n",
      "Batch 1200 | Loss: 1.0820\n",
      "Batch 1300 | Loss: 1.0622\n",
      "Batch 1400 | Loss: 1.0280\n",
      "Batch 1500 | Loss: 1.0956\n",
      "Batch 1600 | Loss: 1.0289\n",
      "Batch 1700 | Loss: 0.9894\n",
      "Batch 1800 | Loss: 1.0594\n",
      "Batch 1900 | Loss: 1.0297\n",
      "Batch 2000 | Loss: 1.0052\n",
      "Epoch: 02, Loss: 1.0349\n",
      "Batch 0 | Loss: 1.0237\n",
      "Batch 100 | Loss: 0.9672\n",
      "Batch 200 | Loss: 1.0279\n",
      "Batch 300 | Loss: 1.0447\n",
      "Batch 400 | Loss: 1.0012\n",
      "Batch 500 | Loss: 1.0539\n",
      "Batch 600 | Loss: 1.0703\n",
      "Batch 700 | Loss: 0.9952\n",
      "Batch 800 | Loss: 1.0142\n",
      "Batch 900 | Loss: 1.0748\n",
      "Batch 1000 | Loss: 1.0610\n",
      "Batch 1100 | Loss: 1.0645\n",
      "Batch 1200 | Loss: 0.9771\n",
      "Batch 1300 | Loss: 0.9735\n",
      "Batch 1400 | Loss: 1.0078\n",
      "Batch 1500 | Loss: 1.0655\n",
      "Batch 1600 | Loss: 0.9429\n",
      "Batch 1700 | Loss: 1.0113\n",
      "Batch 1800 | Loss: 1.0382\n",
      "Batch 1900 | Loss: 1.1292\n",
      "Batch 2000 | Loss: 1.0632\n",
      "Epoch: 03, Loss: 1.0309\n",
      "Batch 0 | Loss: 1.0384\n",
      "Batch 100 | Loss: 0.9470\n",
      "Batch 200 | Loss: 1.0188\n",
      "Batch 300 | Loss: 1.0843\n",
      "Batch 400 | Loss: 1.0686\n",
      "Batch 500 | Loss: 0.9828\n",
      "Batch 600 | Loss: 0.9897\n",
      "Batch 700 | Loss: 0.9394\n",
      "Batch 800 | Loss: 1.0434\n",
      "Batch 900 | Loss: 1.0786\n",
      "Batch 1000 | Loss: 1.0781\n",
      "Batch 1100 | Loss: 1.0566\n",
      "Batch 1200 | Loss: 1.0431\n",
      "Batch 1300 | Loss: 1.0738\n",
      "Batch 1400 | Loss: 1.0417\n",
      "Batch 1500 | Loss: 1.0415\n",
      "Batch 1600 | Loss: 1.0050\n",
      "Batch 1700 | Loss: 0.9358\n",
      "Batch 1800 | Loss: 1.0045\n",
      "Batch 1900 | Loss: 0.9883\n",
      "Batch 2000 | Loss: 0.9767\n",
      "Epoch: 04, Loss: 1.0300\n",
      "Batch 0 | Loss: 0.9730\n",
      "Batch 100 | Loss: 1.0410\n",
      "Batch 200 | Loss: 1.0004\n",
      "Batch 300 | Loss: 0.9865\n",
      "Batch 400 | Loss: 1.0139\n",
      "Batch 500 | Loss: 1.0151\n",
      "Batch 600 | Loss: 1.0002\n",
      "Batch 700 | Loss: 1.0386\n",
      "Batch 800 | Loss: 0.9631\n",
      "Batch 900 | Loss: 1.1161\n",
      "Batch 1000 | Loss: 1.0331\n",
      "Batch 1100 | Loss: 0.9932\n",
      "Batch 1200 | Loss: 1.0487\n",
      "Batch 1300 | Loss: 1.1309\n",
      "Batch 1400 | Loss: 1.1253\n",
      "Batch 1500 | Loss: 1.1014\n",
      "Batch 1600 | Loss: 1.1152\n",
      "Batch 1700 | Loss: 0.9804\n",
      "Batch 1800 | Loss: 1.0608\n",
      "Batch 1900 | Loss: 1.0604\n",
      "Batch 2000 | Loss: 1.0310\n",
      "Epoch: 05, Loss: 1.0289\n",
      "Batch 0 | Loss: 1.0651\n",
      "Batch 100 | Loss: 1.0117\n",
      "Batch 200 | Loss: 0.9795\n",
      "Batch 300 | Loss: 1.0853\n",
      "Batch 400 | Loss: 1.0670\n",
      "Batch 500 | Loss: 0.9766\n",
      "Batch 600 | Loss: 1.0676\n",
      "Batch 700 | Loss: 0.9722\n",
      "Batch 800 | Loss: 1.1002\n",
      "Batch 900 | Loss: 1.0970\n",
      "Batch 1000 | Loss: 0.9866\n",
      "Batch 1100 | Loss: 1.0563\n",
      "Batch 1200 | Loss: 1.0522\n",
      "Batch 1300 | Loss: 1.1070\n",
      "Batch 1400 | Loss: 1.1181\n",
      "Batch 1500 | Loss: 1.0092\n",
      "Batch 1600 | Loss: 1.0831\n",
      "Batch 1700 | Loss: 0.9789\n",
      "Batch 1800 | Loss: 1.0814\n",
      "Batch 1900 | Loss: 1.0273\n",
      "Batch 2000 | Loss: 1.0581\n",
      "Epoch: 06, Loss: 1.0278\n",
      "Batch 0 | Loss: 1.0517\n",
      "Batch 100 | Loss: 1.0454\n",
      "Batch 200 | Loss: 1.0563\n",
      "Batch 300 | Loss: 1.0379\n",
      "Batch 400 | Loss: 1.0196\n",
      "Batch 500 | Loss: 1.0882\n",
      "Batch 600 | Loss: 1.0919\n",
      "Batch 700 | Loss: 0.9815\n",
      "Batch 800 | Loss: 0.9789\n",
      "Batch 900 | Loss: 1.0196\n",
      "Batch 1000 | Loss: 1.0453\n",
      "Batch 1100 | Loss: 0.9550\n",
      "Batch 1200 | Loss: 0.9792\n",
      "Batch 1300 | Loss: 1.0040\n",
      "Batch 1400 | Loss: 1.0800\n",
      "Batch 1500 | Loss: 0.9981\n",
      "Batch 1600 | Loss: 0.9362\n",
      "Batch 1700 | Loss: 1.0075\n",
      "Batch 1800 | Loss: 1.0514\n",
      "Batch 1900 | Loss: 1.0766\n",
      "Batch 2000 | Loss: 0.9884\n",
      "Epoch: 07, Loss: 1.0269\n",
      "Batch 0 | Loss: 0.9664\n",
      "Batch 100 | Loss: 1.0658\n",
      "Batch 200 | Loss: 1.0304\n",
      "Batch 300 | Loss: 0.9659\n",
      "Batch 400 | Loss: 0.9723\n",
      "Batch 500 | Loss: 1.0315\n",
      "Batch 600 | Loss: 1.0122\n",
      "Batch 700 | Loss: 1.0612\n",
      "Batch 800 | Loss: 1.1049\n",
      "Batch 900 | Loss: 1.0129\n",
      "Batch 1000 | Loss: 1.0570\n",
      "Batch 1100 | Loss: 1.0208\n",
      "Batch 1200 | Loss: 0.9918\n",
      "Batch 1300 | Loss: 1.0047\n",
      "Batch 1400 | Loss: 0.9895\n",
      "Batch 1500 | Loss: 1.0122\n",
      "Batch 1600 | Loss: 1.0371\n",
      "Batch 1700 | Loss: 0.9826\n",
      "Batch 1800 | Loss: 1.0747\n",
      "Batch 1900 | Loss: 0.9788\n",
      "Batch 2000 | Loss: 0.9931\n",
      "Epoch: 08, Loss: 1.0256\n",
      "Batch 0 | Loss: 1.0454\n",
      "Batch 100 | Loss: 1.0432\n",
      "Batch 200 | Loss: 1.0184\n",
      "Batch 300 | Loss: 0.9840\n",
      "Batch 400 | Loss: 1.0123\n",
      "Batch 500 | Loss: 1.0456\n",
      "Batch 600 | Loss: 0.9552\n",
      "Batch 700 | Loss: 0.9968\n",
      "Batch 800 | Loss: 1.0216\n",
      "Batch 900 | Loss: 1.0684\n",
      "Batch 1000 | Loss: 1.0045\n",
      "Batch 1100 | Loss: 1.0088\n",
      "Batch 1200 | Loss: 0.9813\n",
      "Batch 1300 | Loss: 1.0269\n",
      "Batch 1400 | Loss: 1.1251\n",
      "Batch 1500 | Loss: 1.0756\n",
      "Batch 1600 | Loss: 1.0969\n",
      "Batch 1700 | Loss: 0.9810\n",
      "Batch 1800 | Loss: 1.1138\n",
      "Batch 1900 | Loss: 1.0386\n",
      "Batch 2000 | Loss: 1.0646\n",
      "Epoch: 09, Loss: 1.0247\n",
      "Batch 0 | Loss: 0.9870\n",
      "Batch 100 | Loss: 0.9907\n",
      "Batch 200 | Loss: 1.0825\n",
      "Batch 300 | Loss: 1.0560\n",
      "Batch 400 | Loss: 0.9789\n",
      "Batch 500 | Loss: 1.0300\n",
      "Batch 600 | Loss: 1.0201\n",
      "Batch 700 | Loss: 0.9496\n",
      "Batch 800 | Loss: 0.9610\n",
      "Batch 900 | Loss: 1.0051\n",
      "Batch 1000 | Loss: 1.0242\n",
      "Batch 1100 | Loss: 0.9960\n",
      "Batch 1200 | Loss: 1.0525\n",
      "Batch 1300 | Loss: 1.0696\n",
      "Batch 1400 | Loss: 1.0159\n",
      "Batch 1500 | Loss: 1.0380\n",
      "Batch 1600 | Loss: 1.0382\n",
      "Batch 1700 | Loss: 1.0229\n",
      "Batch 1800 | Loss: 0.9621\n",
      "Batch 1900 | Loss: 1.0524\n",
      "Batch 2000 | Loss: 1.0645\n",
      "Epoch: 10, Loss: 1.0237\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CDRModel(emb_dim=64).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    alpha = 0.1 \n",
    "    \n",
    "    loader_t_iter = iter(train_loader_t)\n",
    "    \n",
    "    for i, batch_s in enumerate(train_loader_s):\n",
    "        try:\n",
    "            batch_t = next(loader_t_iter)\n",
    "        except StopIteration:\n",
    "            loader_t_iter = iter(train_loader_t)\n",
    "            batch_t = next(loader_t_iter)\n",
    "            \n",
    "        batch_s, batch_t = batch_s.to(device), batch_t.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 1. Forward Pass (Encoder)\n",
    "        z_s_dict = model.encoder(batch_s.x_dict, batch_s.edge_index_dict)\n",
    "        z_t_dict = model.encoder(batch_t.x_dict, batch_t.edge_index_dict)\n",
    "        \n",
    "        # 2. Rating Prediction Loss (Sadece Books Üzerinden)\n",
    "        # LinkNeighborLoader'da seçilen kenarlar 'edge_label_index' içindedir\n",
    "        edge_index_s = batch_s['user', 'interacts', 'item'].edge_label_index\n",
    "        labels_s = batch_s['user', 'interacts', 'item'].edge_label\n",
    "        \n",
    "        preds_s = model.predict(z_s_dict['user'], z_s_dict['item'], edge_index_s)\n",
    "        loss_task = F.mse_loss(preds_s, labels_s)\n",
    "        \n",
    "        # 3. Adversarial Loss (Domain Invariance)\n",
    "        d_s = model.discriminator(z_s_dict['user'], alpha)\n",
    "        d_t = model.discriminator(z_t_dict['user'], alpha)\n",
    "        loss_adv = F.binary_cross_entropy(d_s, torch.zeros_like(d_s)) + \\\n",
    "                   F.binary_cross_entropy(d_t, torch.ones_like(d_t))\n",
    "        \n",
    "        # Toplam Kayıp\n",
    "        loss = loss_task + 0.1 * loss_adv\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(f\"Batch {i} | Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    return total_loss / (i + 1)\n",
    "# Eğitimi Başlat\n",
    "print(f\"Eğitim {device} üzerinde başlıyor...\")\n",
    "for epoch in range(1, 11):\n",
    "    loss = train()\n",
    "    print(f\"Epoch: {epoch:02d}, Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7f4abf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target Domain (Electronics) Tahmini RMSE: 1.1743\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, loader, device, max_batches=500):\n",
    "    model.eval()\n",
    "    total_mse = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(loader):\n",
    "            if i >= max_batches: # Belirli bir batch'ten sonra dur\n",
    "                break\n",
    "                \n",
    "            batch = batch.to(device)\n",
    "            z_dict = model.encoder(batch.x_dict, batch.edge_index_dict)\n",
    "            \n",
    "            edge_index = batch['user', 'interacts', 'item'].edge_label_index\n",
    "            labels = batch['user', 'interacts', 'item'].edge_label\n",
    "            \n",
    "            preds = model.predict(z_dict['user'], z_dict['item'], edge_index)\n",
    "            \n",
    "            total_mse += F.mse_loss(preds, labels, reduction='sum').item()\n",
    "            total_samples += labels.size(0)\n",
    "            \n",
    "    rmse = (total_mse / total_samples) ** 0.5\n",
    "    return rmse\n",
    "\n",
    "# Şimdi daha hızlı çalışacaktır:\n",
    "target_rmse = evaluate_model(model, train_loader_t, device, max_batches=500)\n",
    "print(f\"\\nTarget Domain (Electronics) Tahmini RMSE: {target_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d8ec0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
